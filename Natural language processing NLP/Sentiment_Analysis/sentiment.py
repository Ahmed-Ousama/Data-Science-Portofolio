# -*- coding: utf-8 -*-
"""sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nU8g51nRRKHgWdnJzeTuq44ML2CJ9wG1
"""

from google.colab import drive

drive.mount('/content/drive/')

"""#we start importing important libraries """

import pandas as pd 
import re
import numpy as np
import os
from collections import Counter
import logging
import time
import pickle
import itertools

"""#We have to define the important columns we needs from our data 
The encoding of our data is ISO-8859-1 and we have to defne the encoding while importing our data to avoid errors 
"""

DATASET_COLUMNS = ["target", "ids", "date", "flag", "user", "text"]
DATASET_ENCODING = "ISO-8859-1"
TRAIN_SIZE = 0.8

"""#Be Careful, you have to define the file path you are working on 
Using os library, and i really advise to create a new folder for your project with your dataset inside of the folder
"""

df = pd.read_csv('/content/drive/MyDrive/sentiment /training.1600000.processed.noemoticon.csv', encoding =DATASET_ENCODING , names=DATASET_COLUMNS)

"""#Let's view our data"""

df.head(10)

"""As we can see from target columns, the negative is labeled as 0 and positive is labeled as 4 but we have to decode int into str labels, as follow
The Target column represents the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)
The ID column represents the id of the tweet 
The Date column represents the date of the tweet 
The Flagcolumn represents the query (lyx) and If there is no query, then this value is NO_QUERY
The User column represents the user that tweeted (robotickilldozr)
The Flagcolumn represents the text of the tweet (Lyx is cool)
"""

decode_map = {0: "NEGATIVE", 2: "NEUTRAL", 4: "POSITIVE"}
def decode_sentiment(label):
    return decode_map[int(label)]
df.target = df.target.apply(lambda x: decode_sentiment(x))

"""Before working on our data we have to make sure that if our dataset is balanced or not """

from collections import Counter
import matplotlib.pyplot as plt
target_cnt = Counter(df.target)
plt.figure(figsize=(16,8))
plt.bar(target_cnt.keys(), target_cnt.values())
plt.title("Dataset labels distribuition")

"""Great, It is balanced

Let's do some preprocess for our data before building our model and make the understanding process of data much easier 
- Stop words are considered in language to make sense for us but for machines, there is no need to keep them beside it will cause a disturbance.
- stemming is used to get the origin of words.
- Special characters like $,#, & and dots. Will cause disturbance for our model so we need to remove those characters using RE
"""

# nltk
import nltk
from nltk.corpus import stopwords
from  nltk.stem import SnowballStemmer

"""We collect those functions to work in one function under preprocessing title :

"""

def preprocess(text, stem=False):
    # Remove link,user and special characters
    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()
    tokens = []
    for token in text.split():
        if token not in stop_words:
            if stem:
                tokens.append(stemmer.stem(token))
            else:
                tokens.append(token)
    return " ".join(tokens)

TEXT_CLEANING_RE = "@\S+|https?:\S+|http?:\S|[^A-Za-z0-9]+"
nltk.download('stopwords')
# TEXT CLENAING
TEXT_CLEANING_RE = "@\S+|https?:\S+|http?:\S|[^A-Za-z0-9]+"

"""Let's import them """

stop_words = stopwords.words("english")
stemmer = SnowballStemmer("english")

"""Now, we can clean our data"""

import re
df.text = df.text.apply(lambda x: preprocess(x))

"""Let's see if our text column is preprocessed and cleaned or not """

df.head(3)

"""Great work

Now for building our model we have to split our dataset for training data and testing data with an 80% percent for training data and 20% for testing data
"""

from sklearn.model_selection import train_test_split
TRAIN_SIZE = 0.8
df_train, df_test = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=42)
print("TRAIN size:", len(df_train))
print("TEST size:", len(df_test))

"""Then we have the results of TRAIN size: 1280000 and  TEST size: 320000.

Let's view our train data
"""

print(df_train.head())

"""We have to build our word to vector model first so we can define the similarity of words for each other

Let's split our words and collect them into one list called documents
"""

documents = [text.split() for text in df_train.text]

"""Import gensim library to build our model easily"""

import gensim
from gensim.models import KeyedVectors
from gensim import models
# WORD2VEC 
W2V_SIZE = 300
W2V_WINDOW = 7
W2V_EPOCH = 32
W2V_MIN_COUNT = 10

"""Let's define the structure of our w2v model"""

w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, 
                                            window=W2V_WINDOW, 
                                            min_count=W2V_MIN_COUNT, 
                                            workers=8)

"""Let's build our w2v model """

w2v_model.build_vocab(documents)

"""Well done.

Let's test our model now
"""

w2v_model.most_similar("love")

"""#Tokenizer
let's give each word we have in our dataset a number using tokenizer 
"""

from keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(df_train.text)

vocab_size = len(tokenizer.word_index) + 1
print("Total words", vocab_size)

"""Well, let's construct our sentences again using pad_sequence"""

from keras.preprocessing.sequence import pad_sequences
SEQUENCE_LENGTH = 300
x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text), maxlen=SEQUENCE_LENGTH)
x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text), maxlen=SEQUENCE_LENGTH)

"""let's see what is the result of our work now.                                                    
0 represents the null space as we define our sequence length as 300 words to acquire all of our dataset sentence
"""

print(x_train[3])

"""let's add the neutral label to our labels list """

NEUTRAL = "NEUTRAL"
labels = df_train.target.unique().tolist()
labels.append(NEUTRAL)
labels

"""#Encoding """

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
encoder.fit(df_train.target.tolist())

y_train = encoder.transform(df_train.target.tolist())
y_test = encoder.transform(df_test.target.tolist())

y_train = y_train.reshape(-1,1)
y_test = y_test.reshape(-1,1)

print("y_train",y_train.shape)
print("y_test",y_test.shape)

print("x_train", x_train.shape)
print("y_train", y_train.shape)
print()
print("x_test", x_test.shape)
print("y_test", y_test.shape)

"""#Embedding """

import numpy as np
W2V_SIZE = 300
W2V_WINDOW = 7
W2V_EPOCH = 32
W2V_MIN_COUNT = 10
embedding_matrix = np.zeros((vocab_size, W2V_SIZE))
for word, i in tokenizer.word_index.items():
  if word in w2v_model.wv:
    embedding_matrix[i] = w2v_model.wv[word]
print(embedding_matrix.shape)

"""Finally, we finished our preprocessing process

#Time for building our model
"""

from keras.layers import *
embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)
# KERAS
SEQUENCE_LENGTH = 300
EPOCHS = 8
BATCH_SIZE = 1024

from keras.models import Sequential
model = Sequential()
model.add(embedding_layer)
model.add(Dropout(0.65))
model.add(Conv1D(64, 5, padding='valid', activation='relu', strides=1))
model.add(MaxPooling1D(pool_size=4))
model.add(Bidirectional(LSTM(100,dropout=0.6)))
model.add(Dense(50, activation='relu'))
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.summary()

model.compile(loss='binary_crossentropy',
              optimizer="nadam",
              metrics=['accuracy'])

from keras.callbacks import *
callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),
              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]

BATCH_SIZE = 800
EPOCHS = 12
history = model.fit(x_train, y_train,
                    batch_size=BATCH_SIZE,
                    epochs=EPOCHS,
                    validation_split=0.2,
                    verbose=1,
                    callbacks=callbacks)

"""Notes:                                                                                        
After 12 epoch, the accuracy will be between 75:77.                                                                                                                            
Training the model longer will give you better results.

let's evaluate our model using test_data we created
"""

score = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)
print()
print("ACCURACY:",score[1])
print("LOSS:",score[0])

"""#It is time for detecting if our model is working well, underfitting or overfitting."""

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
 
epochs = range(len(acc))
 
plt.plot(epochs, acc, 'b', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
 
plt.figure()
 
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
 
plt.show()

"""Our model is working really well

let's save our model
"""

KERAS_MODEL = "model.h5"
WORD2VEC_MODEL = "model.w2v"
TOKENIZER_MODEL = "tokenizer.pkl"
ENCODER_MODEL = "encoder.pkl"
model.save(KERAS_MODEL)
w2v_model.save(WORD2VEC_MODEL)
pickle.dump(tokenizer, open(TOKENIZER_MODEL, "wb"), protocol=0)
pickle.dump(encoder, open(ENCODER_MODEL, "wb"), protocol=0)

"""#Predict

Let's decode our sentiment first
"""

# SENTIMENT
POSITIVE = "POSITIVE"
NEGATIVE = "NEGATIVE"
NEUTRAL = "NEUTRAL"
SENTIMENT_THRESHOLDS = (0.4, 0.7)


def decode_sentiment(score, include_neutral=True):
    if include_neutral:        
        label = NEUTRAL
        if score <= SENTIMENT_THRESHOLDS[0]:
            label = NEGATIVE
        elif score >= SENTIMENT_THRESHOLDS[1]:
            label = POSITIVE

        return label
    else:
        return NEGATIVE if score < 0.5 else POSITIVE

def predict(text, include_neutral=True):
    start_at = time.time()
    # Tokenize text
    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)
    # Predict
    score = model.predict([x_test])[0]
    # Decode sentiment
    label = decode_sentiment(score, include_neutral=include_neutral)

    return {"label": label, "score": float(score),
       "elapsed_time": time.time()-start_at}

predict('Write here the sentence u want to test for review sentiment')

"""#confusion Matrix"""

y_pred_1d = []
y_test_1d = list(df_test.target)
scores = model.predict(x_test, verbose=1, batch_size=8000)
y_pred_1d = [decode_sentiment(score, include_neutral=False) for score in scores]

def plot_confusion_matrix(cm, classes,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """

    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title, fontsize=30)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=90, fontsize=22)
    plt.yticks(tick_marks, classes, fontsize=22)

    fmt = '.2f'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label', fontsize=25)
    plt.xlabel('Predicted label', fontsize=25)

cnf_matrix = confusion_matrix(y_test_1d, y_pred_1d)
plt.figure(figsize=(12,12))
plot_confusion_matrix(cnf_matrix, classes=df_train.target.unique(), title="Confusion matrix")
plt.show()